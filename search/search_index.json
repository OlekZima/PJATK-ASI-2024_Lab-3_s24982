{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to College Distances dataset exploration The College Distances dataset experiment aims to predict students' academic scores based on various demographic, socioeconomic, and educational factors. The dataset includes information such as parental education levels, income, gender, home location, distance to college, and other indicators that might influence academic performance. Objective The primary objective of this experiment is to train a predictive model that accurately forecasts student scores based on a range of input features. By analyzing these relationships, the model can help identify the most influential factors affecting academic success and aid in targeted educational interventions. Dataset Overview The dataset includes variables such as: Distance to College : How far the student lives from their college, potentially impacting commute time and study hours. Parental Education : Separate indicators for father\u2019s and mother\u2019s college education, representing family academic background. Income Level : Represents family income, indicating potential access to educational resources. Home Environment : Whether the student lives in a supportive home environment. Urban vs. Rural : Categorizes students based on urban or rural residence, which can affect access to resources. Other Demographic Variables : Gender, ethnicity, and region to capture broader socio-demographic factors. Experiment Approach Data Preprocessing: Clean the dataset by handling missing values, encoding categorical features, and scaling numerical values. Feature Engineering: Transform variables, such as converting categorical variables into binary or one-hot encoded features. Model Development: Train and evaluate multiple machine learning models (e.g., linear regression, decision trees, or neural networks) to find the best predictive approach for student scores. Performance Evaluation: Use metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), or R\u00b2 to assess model accuracy. Expected Outcome The expected outcome is a model that can reliably predict student scores based on the provided features, along with insights into which factors have the greatest impact on academic success. This experiment could potentially aid educational policymakers in identifying at-risk students and tailoring resources to optimize learning outcomes.","title":"Home"},{"location":"#welcome-to-college-distances-dataset-exploration","text":"The College Distances dataset experiment aims to predict students' academic scores based on various demographic, socioeconomic, and educational factors. The dataset includes information such as parental education levels, income, gender, home location, distance to college, and other indicators that might influence academic performance.","title":"Welcome to College Distances dataset exploration"},{"location":"#objective","text":"The primary objective of this experiment is to train a predictive model that accurately forecasts student scores based on a range of input features. By analyzing these relationships, the model can help identify the most influential factors affecting academic success and aid in targeted educational interventions.","title":"Objective"},{"location":"#dataset-overview","text":"The dataset includes variables such as: Distance to College : How far the student lives from their college, potentially impacting commute time and study hours. Parental Education : Separate indicators for father\u2019s and mother\u2019s college education, representing family academic background. Income Level : Represents family income, indicating potential access to educational resources. Home Environment : Whether the student lives in a supportive home environment. Urban vs. Rural : Categorizes students based on urban or rural residence, which can affect access to resources. Other Demographic Variables : Gender, ethnicity, and region to capture broader socio-demographic factors.","title":"Dataset Overview"},{"location":"#experiment-approach","text":"Data Preprocessing: Clean the dataset by handling missing values, encoding categorical features, and scaling numerical values. Feature Engineering: Transform variables, such as converting categorical variables into binary or one-hot encoded features. Model Development: Train and evaluate multiple machine learning models (e.g., linear regression, decision trees, or neural networks) to find the best predictive approach for student scores. Performance Evaluation: Use metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), or R\u00b2 to assess model accuracy.","title":"Experiment Approach"},{"location":"#expected-outcome","text":"The expected outcome is a model that can reliably predict student scores based on the provided features, along with insights into which factors have the greatest impact on academic success. This experiment could potentially aid educational policymakers in identifying at-risk students and tailoring resources to optimize learning outcomes.","title":"Expected Outcome"},{"location":"data_explore/","text":"Data explore on College Distance dataset Introduction Given the dataset with cross-section data from the High School and Beyond survey conducted by the Department of Education (USA) in 1980, with a follow-up in 1986. The survey included students from approximately 1,100 high schools. The goal is to create an ML model that will predict the base year composite tets score or just the score feature in the dataset. Details All given information comes from this page Rouse (1995) computed years of education by assigning 12 years to all members of the senior class. Each additional year of secondary education counted as a one year. Students with vocational degrees were assigned 13 years, AA degrees were assigned 14 years, BA degrees were assigned 16 years, those with some graduate education were assigned 17 years, and those with a graduate degree were assigned 18 years. Stock and Watson (2007) provide separate data files for the students from Western states and the remaining students. CollegeDistance includes both data sets, subsets are easily obtained (see also examples). Source Online complements to Stock and Watson (2007). References Rouse, C.E. (1995). Democratization or Diversion? The Effect of Community Colleges on Educational Attainment. Journal of Business & Economic Statistics, 12, 217\u2013224. Stock, J.H. and Watson, M.W. (2007). Introduction to Econometrics, 2nd ed. Boston: Addison Wesley. Data format A data frame containing 4739 observations based on 14 variables (15 including rownames feature). Features explained: gender : factor indicating gender. Type: categorical Possible values: female or male . ethnicity : factor indicating ethnicity. Type: categorical Possible values: afam (African-American), hispanic or other . score : base year composite test score. These are achievement tests given to high school seniors in the sample. Type: float Possible values: non-negative floating point number. fcollege : factor. Is the father a college graduate? Type: categorical Possible values: yes or no . mcollege : factor. Is the mother a college graduate? Type: categorical Possible values: yes or no . home : factor. Does the family own their home? Type: categorical Possible values: yes or no . urban : factor. Is the school in an urban area? Type: categorical Possible values: yes or no . unemp : county unemployment rate in 1980. Type: float Possible values: non-negative floating point number. wage : state hourly wage in manufacturing in 1980. Type: float Possible values: non-negative floating point number. distance : distance from 4-year college (in 10 miles). Type: float Possible values: non-negative floating point number. tuition : average state 4-year college tuition (in 1000 USD). Type: float Possible values: non-negative floating point number. education : number of years of education. Type: int Possible values: non-negative integer. income : factor. Is the family income above USD 25,000 per year? Type: categorical Possible values: low or high . region : factor indicating region. Type: categorical Possible values: west or other . First look at the data rownames gender ethnicity score fcollege mcollege home urban unemp wage distance tuition education income region 1 male other 39.150001525878906 yes no yes yes 6.199999809265137 8.09000015258789 0.20000000298023224 0.8891500234603882 12 high other 2 female other 48.869998931884766 no no yes yes 6.199999809265137 8.09000015258789 0.20000000298023224 0.8891500234603882 12 low other 3 male other 48.7400016784668 no no yes yes 6.199999809265137 8.09000015258789 0.20000000298023224 0.8891500234603882 12 low other 4 male afam 40.400001525878906 no no yes yes 6.199999809265137 8.09000015258789 0.20000000298023224 0.8891500234603882 12 low other 5 female other 40.47999954223633 no no no yes 5.599999904632568 8.09000015258789 0.4000000059604645 0.8891500234603882 13 low other Amount of empty cells: 1 data_df . isnull () . sum () Code output: rownames 0 gender 0 ethnicity 0 score 0 fcollege 0 mcollege 0 home 0 urban 0 unemp 0 wage 0 distance 0 tuition 0 education 0 income 0 region 0 Dataset contains no empty or null cells. Distributions of the features Plot with distribution of distances feature Plot with distribution of education feature Plot with distribution of score feature Plot with distribution of tuition feature Plot with distribution of unemp feature Plot with distribution of wage feature Box plots of the features Box plot of the distance feature Box plot of the education feature Box plot of the score feature Box plot of the tuition feature Box plot of the unemp feature Box plot of the wage feature Note Statistics of the rownames feature is missing because it only represents index of the row. Therefore, it is safe to drop this feature in this project.","title":"Data Explore"},{"location":"data_explore/#data-explore-on-college-distance-dataset","text":"","title":"Data explore on College Distance dataset"},{"location":"data_explore/#introduction","text":"Given the dataset with cross-section data from the High School and Beyond survey conducted by the Department of Education (USA) in 1980, with a follow-up in 1986. The survey included students from approximately 1,100 high schools. The goal is to create an ML model that will predict the base year composite tets score or just the score feature in the dataset.","title":"Introduction"},{"location":"data_explore/#details","text":"All given information comes from this page Rouse (1995) computed years of education by assigning 12 years to all members of the senior class. Each additional year of secondary education counted as a one year. Students with vocational degrees were assigned 13 years, AA degrees were assigned 14 years, BA degrees were assigned 16 years, those with some graduate education were assigned 17 years, and those with a graduate degree were assigned 18 years. Stock and Watson (2007) provide separate data files for the students from Western states and the remaining students. CollegeDistance includes both data sets, subsets are easily obtained (see also examples).","title":"Details"},{"location":"data_explore/#source","text":"Online complements to Stock and Watson (2007).","title":"Source"},{"location":"data_explore/#references","text":"Rouse, C.E. (1995). Democratization or Diversion? The Effect of Community Colleges on Educational Attainment. Journal of Business & Economic Statistics, 12, 217\u2013224. Stock, J.H. and Watson, M.W. (2007). Introduction to Econometrics, 2nd ed. Boston: Addison Wesley.","title":"References"},{"location":"data_explore/#data-format","text":"A data frame containing 4739 observations based on 14 variables (15 including rownames feature). Features explained: gender : factor indicating gender. Type: categorical Possible values: female or male . ethnicity : factor indicating ethnicity. Type: categorical Possible values: afam (African-American), hispanic or other . score : base year composite test score. These are achievement tests given to high school seniors in the sample. Type: float Possible values: non-negative floating point number. fcollege : factor. Is the father a college graduate? Type: categorical Possible values: yes or no . mcollege : factor. Is the mother a college graduate? Type: categorical Possible values: yes or no . home : factor. Does the family own their home? Type: categorical Possible values: yes or no . urban : factor. Is the school in an urban area? Type: categorical Possible values: yes or no . unemp : county unemployment rate in 1980. Type: float Possible values: non-negative floating point number. wage : state hourly wage in manufacturing in 1980. Type: float Possible values: non-negative floating point number. distance : distance from 4-year college (in 10 miles). Type: float Possible values: non-negative floating point number. tuition : average state 4-year college tuition (in 1000 USD). Type: float Possible values: non-negative floating point number. education : number of years of education. Type: int Possible values: non-negative integer. income : factor. Is the family income above USD 25,000 per year? Type: categorical Possible values: low or high . region : factor indicating region. Type: categorical Possible values: west or other .","title":"Data format"},{"location":"data_explore/#first-look-at-the-data","text":"rownames gender ethnicity score fcollege mcollege home urban unemp wage distance tuition education income region 1 male other 39.150001525878906 yes no yes yes 6.199999809265137 8.09000015258789 0.20000000298023224 0.8891500234603882 12 high other 2 female other 48.869998931884766 no no yes yes 6.199999809265137 8.09000015258789 0.20000000298023224 0.8891500234603882 12 low other 3 male other 48.7400016784668 no no yes yes 6.199999809265137 8.09000015258789 0.20000000298023224 0.8891500234603882 12 low other 4 male afam 40.400001525878906 no no yes yes 6.199999809265137 8.09000015258789 0.20000000298023224 0.8891500234603882 12 low other 5 female other 40.47999954223633 no no no yes 5.599999904632568 8.09000015258789 0.4000000059604645 0.8891500234603882 13 low other","title":"First look at the data"},{"location":"data_explore/#amount-of-empty-cells","text":"1 data_df . isnull () . sum () Code output: rownames 0 gender 0 ethnicity 0 score 0 fcollege 0 mcollege 0 home 0 urban 0 unemp 0 wage 0 distance 0 tuition 0 education 0 income 0 region 0 Dataset contains no empty or null cells.","title":"Amount of empty cells:"},{"location":"data_explore/#distributions-of-the-features","text":"Plot with distribution of distances feature Plot with distribution of education feature Plot with distribution of score feature Plot with distribution of tuition feature Plot with distribution of unemp feature Plot with distribution of wage feature","title":"Distributions of the features"},{"location":"data_explore/#box-plots-of-the-features","text":"Box plot of the distance feature Box plot of the education feature Box plot of the score feature Box plot of the tuition feature Box plot of the unemp feature Box plot of the wage feature Note Statistics of the rownames feature is missing because it only represents index of the row. Therefore, it is safe to drop this feature in this project.","title":"Box plots of the features"},{"location":"feature_engineering/","text":"Feature Engineering Data encoding Categorical values features Categorical features was coded as: Changes gender ethnicity fcollege/mcollege home urban income region Renamed to `gender_is_female` `female` : True `male` : False OneHotEncoded 3 features were created: `ethnicity_afam` `ethnicity_hispanic` and `ethnicity_other` Renamed to `is_fcollege` / `is_mcollege` `yes` : True `no` : False Renamed to `is_home` `yes` : True `no` : False Renamed to `is_urban` `yes` : True `no` : False Renamed to `is_high_income` `high` : True `low` : False Renamed to `is_region_west` `west` : True `other` : False Correlation Heatmap After transforming categorical features into numerical it is possible to generate proper heatmap correlation between features. Correlation Heatmap generating Python plt . figure ( figsize = ( 10 , 8 )) corr = data_df . corr () corr_rounded = corr . round ( 2 ) sns . heatmap ( corr , annot = corr_rounded , cmap = \"coolwarm\" ) plt . title ( \"Correlation Heatmap\" ) plt . show () Output: On this heatmap, we see that the most correlation for the score target feature comes from education , ethnicity_other , and is_fcollege . This means that: A higher amount of total education years implies the highest score, which makes sense. If the ethnicity is other (not African-American or Hispanic), the score is higher. If the is_fcollege is set to True (father finished college), the score will be higher. Similarly, there is a correlation between is_mcollege (mother finished college), but it is lower than the previous one. Pairplot Pairplot generating Python sns . pairplot ( data_df ) plt . show () Output: Note This plot is enormous, so to examine it just do a right-click and \"Open image in new tab\". This plot just shows the correlation, but in slightly \"other way\". So the meaning is the same. Scaling features Features needs to be scaled in order to achieve the best result from our model. As a scaler we'll use StandardScaler from sklearn. Scaling data Python data_df [ \"education\" ] = data_df [ \"education\" ] . astype ( float ) columns_to_scale = data_df . select_dtypes ( include = [ \"float64\" ]) . columns scaler = StandardScaler () data_df [ columns_to_scale ] = scaler . fit_transform ( data_df [ columns_to_scale ]) print ( data_df . head ()) education type was changed to float, so one hot encoded features will stay the same as before. Result (only first two rows) of the scaling: gender_is_female score is_fcollege is_mcollege is_home is_urban unemp wage distance tuition education is_high_income is_region_west ethnicity_afam ethnicity_hispanic ethnicity_other False -1.349160 True False True True -0.505635 -1.050324 -0.697845 0.219584 -1.010536 True False 0 0 1 True -0.232046 False False True True -0.505635 -1.050324 -0.697845 0.219584 -1.010536 False False 0 0 1 Train test split Before training our model data should be split into train and test. We'll use pandas masks for that. Train test split Python msk = np . random . rand ( len ( data_df )) < 0.8 train_df = data_df [ msk ] test_df = data_df [ ~ msk ] print ( train_df . shape , test_df . shape ) Output: (3814, 16) (925, 16)","title":"Feature Engineering"},{"location":"feature_engineering/#feature-engineering","text":"","title":"Feature Engineering"},{"location":"feature_engineering/#data-encoding","text":"","title":"Data encoding"},{"location":"feature_engineering/#categorical-values-features","text":"Categorical features was coded as: Changes gender ethnicity fcollege/mcollege home urban income region Renamed to `gender_is_female` `female` : True `male` : False OneHotEncoded 3 features were created: `ethnicity_afam` `ethnicity_hispanic` and `ethnicity_other` Renamed to `is_fcollege` / `is_mcollege` `yes` : True `no` : False Renamed to `is_home` `yes` : True `no` : False Renamed to `is_urban` `yes` : True `no` : False Renamed to `is_high_income` `high` : True `low` : False Renamed to `is_region_west` `west` : True `other` : False","title":"Categorical values features"},{"location":"feature_engineering/#correlation-heatmap","text":"After transforming categorical features into numerical it is possible to generate proper heatmap correlation between features. Correlation Heatmap generating Python plt . figure ( figsize = ( 10 , 8 )) corr = data_df . corr () corr_rounded = corr . round ( 2 ) sns . heatmap ( corr , annot = corr_rounded , cmap = \"coolwarm\" ) plt . title ( \"Correlation Heatmap\" ) plt . show () Output: On this heatmap, we see that the most correlation for the score target feature comes from education , ethnicity_other , and is_fcollege . This means that: A higher amount of total education years implies the highest score, which makes sense. If the ethnicity is other (not African-American or Hispanic), the score is higher. If the is_fcollege is set to True (father finished college), the score will be higher. Similarly, there is a correlation between is_mcollege (mother finished college), but it is lower than the previous one.","title":"Correlation Heatmap"},{"location":"feature_engineering/#pairplot","text":"Pairplot generating Python sns . pairplot ( data_df ) plt . show () Output: Note This plot is enormous, so to examine it just do a right-click and \"Open image in new tab\". This plot just shows the correlation, but in slightly \"other way\". So the meaning is the same.","title":"Pairplot"},{"location":"feature_engineering/#scaling-features","text":"Features needs to be scaled in order to achieve the best result from our model. As a scaler we'll use StandardScaler from sklearn. Scaling data Python data_df [ \"education\" ] = data_df [ \"education\" ] . astype ( float ) columns_to_scale = data_df . select_dtypes ( include = [ \"float64\" ]) . columns scaler = StandardScaler () data_df [ columns_to_scale ] = scaler . fit_transform ( data_df [ columns_to_scale ]) print ( data_df . head ()) education type was changed to float, so one hot encoded features will stay the same as before. Result (only first two rows) of the scaling: gender_is_female score is_fcollege is_mcollege is_home is_urban unemp wage distance tuition education is_high_income is_region_west ethnicity_afam ethnicity_hispanic ethnicity_other False -1.349160 True False True True -0.505635 -1.050324 -0.697845 0.219584 -1.010536 True False 0 0 1 True -0.232046 False False True True -0.505635 -1.050324 -0.697845 0.219584 -1.010536 False False 0 0 1","title":"Scaling features"},{"location":"feature_engineering/#train-test-split","text":"Before training our model data should be split into train and test. We'll use pandas masks for that. Train test split Python msk = np . random . rand ( len ( data_df )) < 0.8 train_df = data_df [ msk ] test_df = data_df [ ~ msk ] print ( train_df . shape , test_df . shape ) Output: (3814, 16) (925, 16)","title":"Train test split"},{"location":"model_training/","text":"Creating a model Approach As main approach to create a model pycaret library was chosen. The reason is pycaret provides structured and easy API to create, evaluate and tune model. Setting up a pycaret pycaret setup Python reg = setup ( data_df , target = \"score\" , test_data = test_df , remove_outliers = True , fold_strategy = \"kfold\" , fold = 15 , session_id = 2137 , fold_shuffle = True , use_gpu = True , normalize = True , index = False ) best_model = compare_models () Description: data_df : data. target : target feature to predict. test_df : because we split data on our own, we can pass our test data to pycaret. remove_outliers : removes statistical outliers from the data. fold_strategy : The method of folds. fold_shuffle : Does folds needs to be shuffled. fold : cross-validation for selected model. session_id : allows to replicate experiment. normalize : does normalization should be applied: use_gpu : allows pycaret to use gpu to speed things up. index : Indicates whether data has an index column. Output: lots of logs Model MAE MSE RMSE R2 RMSLE MAPE TT (Sec) gbr Gradient Boosting Regressor 0.6551 0.6549 0.8085 0.3407 0.3708 1.9342 0.2800 br Bayesian Ridge 0.6616 0.6578 0.8103 0.3378 0.3807 1.9395 0.1133 Note Given output show only two the best models. Full output table is much larger. The table shows us, that the best model for this dataset with described analysis and preprocessing is Bayesian Ridge . Best results in all metrics. Model evaluation Fold MAE MSE RMSE R2 RMSLE MAPE 0 0.6477 0.6644 0.8151 0.3104 0.3651 1.3917 1 0.6265 0.6169 0.7854 0.4017 0.3688 1.5117 2 0.6315 0.6061 0.7785 0.4055 0.3722 1.5609 3 0.6703 0.6709 0.8191 0.3111 0.3684 2.0291 4 0.6599 0.6467 0.8041 0.3387 0.3938 1.7867 5 0.6202 0.6732 0.8205 0.3154 0.3562 1.5384 6 0.6903 0.6919 0.8318 0.2701 0.3808 1.7269 7 0.6466 0.6363 0.7977 0.3843 0.3434 1.6471 8 0.6668 0.6636 0.8146 0.3884 0.3865 1.9079 9 0.6010 0.5180 0.7197 0.4116 0.3529 3.1464 10 0.6446 0.6047 0.7776 0.3992 0.3720 3.4132 11 0.6676 0.6908 0.8311 0.3463 0.3898 1.8291 12 0.6585 0.6681 0.8174 0.2165 0.3535 1.3079 13 0.7243 0.7756 0.8807 0.3356 0.4085 2.2211 14 0.6706 0.6969 0.8348 0.2763 0.3498 1.9954 Mean 0.6551 0.6549 0.8085 0.3407 0.3708 1.9342 Std 0.0290 0.0550 0.0344 0.0563 0.0178 0.5815 Model diagram However... Our model is overfitted to the train dataset, which is huge problem, because usability of the model is zero . Therefore, tuning is needed. Tuning the model Python tuned_model = tune_model ( model , n_iter = 50 , early_stopping = True , optimize = \"R2\" , choose_better = True ) And after tuning we can visualise our data performance. Residuals Validation Curve Features importance Prediction Error Afterwords Model is located model.pkl in the GH actions artifacts. All details and logs of the experiments can be found in the actions artifacts. Look for logs.txt (pycaret logs) and logging.txt (whole experiment logs). As we can see in the Feature Importance plot, the education has the bigger impact on the end score for the students. Moral? Study hard and tests will be passed! \ud83d\ude42","title":"Creating a model"},{"location":"model_training/#creating-a-model","text":"","title":"Creating a model"},{"location":"model_training/#approach","text":"As main approach to create a model pycaret library was chosen. The reason is pycaret provides structured and easy API to create, evaluate and tune model.","title":"Approach"},{"location":"model_training/#setting-up-a-pycaret","text":"pycaret setup Python reg = setup ( data_df , target = \"score\" , test_data = test_df , remove_outliers = True , fold_strategy = \"kfold\" , fold = 15 , session_id = 2137 , fold_shuffle = True , use_gpu = True , normalize = True , index = False ) best_model = compare_models () Description: data_df : data. target : target feature to predict. test_df : because we split data on our own, we can pass our test data to pycaret. remove_outliers : removes statistical outliers from the data. fold_strategy : The method of folds. fold_shuffle : Does folds needs to be shuffled. fold : cross-validation for selected model. session_id : allows to replicate experiment. normalize : does normalization should be applied: use_gpu : allows pycaret to use gpu to speed things up. index : Indicates whether data has an index column. Output: lots of logs Model MAE MSE RMSE R2 RMSLE MAPE TT (Sec) gbr Gradient Boosting Regressor 0.6551 0.6549 0.8085 0.3407 0.3708 1.9342 0.2800 br Bayesian Ridge 0.6616 0.6578 0.8103 0.3378 0.3807 1.9395 0.1133 Note Given output show only two the best models. Full output table is much larger. The table shows us, that the best model for this dataset with described analysis and preprocessing is Bayesian Ridge . Best results in all metrics.","title":"Setting up a pycaret"},{"location":"model_training/#model-evaluation","text":"Fold MAE MSE RMSE R2 RMSLE MAPE 0 0.6477 0.6644 0.8151 0.3104 0.3651 1.3917 1 0.6265 0.6169 0.7854 0.4017 0.3688 1.5117 2 0.6315 0.6061 0.7785 0.4055 0.3722 1.5609 3 0.6703 0.6709 0.8191 0.3111 0.3684 2.0291 4 0.6599 0.6467 0.8041 0.3387 0.3938 1.7867 5 0.6202 0.6732 0.8205 0.3154 0.3562 1.5384 6 0.6903 0.6919 0.8318 0.2701 0.3808 1.7269 7 0.6466 0.6363 0.7977 0.3843 0.3434 1.6471 8 0.6668 0.6636 0.8146 0.3884 0.3865 1.9079 9 0.6010 0.5180 0.7197 0.4116 0.3529 3.1464 10 0.6446 0.6047 0.7776 0.3992 0.3720 3.4132 11 0.6676 0.6908 0.8311 0.3463 0.3898 1.8291 12 0.6585 0.6681 0.8174 0.2165 0.3535 1.3079 13 0.7243 0.7756 0.8807 0.3356 0.4085 2.2211 14 0.6706 0.6969 0.8348 0.2763 0.3498 1.9954 Mean 0.6551 0.6549 0.8085 0.3407 0.3708 1.9342 Std 0.0290 0.0550 0.0344 0.0563 0.0178 0.5815 Model diagram","title":"Model evaluation"},{"location":"model_training/#however","text":"Our model is overfitted to the train dataset, which is huge problem, because usability of the model is zero . Therefore, tuning is needed. Tuning the model Python tuned_model = tune_model ( model , n_iter = 50 , early_stopping = True , optimize = \"R2\" , choose_better = True ) And after tuning we can visualise our data performance. Residuals Validation Curve Features importance Prediction Error","title":"However..."},{"location":"model_training/#afterwords","text":"Model is located model.pkl in the GH actions artifacts. All details and logs of the experiments can be found in the actions artifacts. Look for logs.txt (pycaret logs) and logging.txt (whole experiment logs). As we can see in the Feature Importance plot, the education has the bigger impact on the end score for the students. Moral? Study hard and tests will be passed! \ud83d\ude42","title":"Afterwords"}]}